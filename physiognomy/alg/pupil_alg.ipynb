{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMU0F2ssUkfk7wKH4ZlfyJo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeonwooCH/Final_physiognomy_palmistry/blob/main/pupil_alg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CevuAhHOLhZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class FaceParsingDataset(Dataset):\n",
        "    def __init__(self, root_path, mode='test'):\n",
        "        self.img_dir = os.path.join(root_path, mode, 'images')\n",
        "        self.mask_dir = os.path.join(root_path, mode, 'masks')\n",
        "        # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ë¡œë“œ\n",
        "        self.file_list = [f for f in sorted(os.listdir(self.img_dir)) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.file_list[idx]\n",
        "\n",
        "        # íŒŒì¼ëª…ì—ì„œ í™•ì¥ìë¥¼ ì œì™¸í•œ ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: '0174' -> '0174_grayscale.png')\n",
        "        base_name = os.path.splitext(img_name)[0]\n",
        "        mask_name = f\"{base_name}_grayscale.png\"\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
        "\n",
        "        # ì´ë¯¸ì§€ ë° ë§ˆìŠ¤í¬ ë¡œë“œ\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        # í…ì„œ ë³€í™˜: [C, H, W] í˜•íƒœ ë° 0~1 ì •ê·œí™”\n",
        "        img_tensor = torch.from_numpy(np.array(img)).permute(2, 0, 1).float() / 255.0\n",
        "        mask_np = np.array(mask)\n",
        "\n",
        "        return img_tensor, mask_np, img_name\n",
        "\n",
        "# ê²½ë¡œ ì„¤ì • (ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì • í•„ìš”)\n",
        "BASE_PATH = '/content/drive/MyDrive/c. Final_Team/Split_dataset'\n",
        "dataset = FaceParsingDataset(BASE_PATH, mode='test')\n",
        "print(f\"Dataset ë¡œë“œ ì™„ë£Œ: {len(dataset)} ê°œì˜ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "id": "n_2dd82SL0KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3M6ilgyKyAr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "# 1. ëˆˆë™ì ê²€ì¶œ í•¨ìˆ˜ (ê²€ì€ ë¶€ë¶„ì„ ê½‰ ì±„ìš°ëŠ” ë¡œì§)\n",
        "def detect_pupils_natural_fixed(img_tensor, gt_mask):\n",
        "    # í…ì„œë¥¼ ë„˜íŒŒì´ë¡œ ë³€í™˜\n",
        "    img_np = (img_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
        "    gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # ëŒ€ë¹„ ê°•í™” (ëˆˆë™ì ì‹ë³„ë ¥ ê·¹ëŒ€í™”)\n",
        "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
        "    enhanced_gray = clahe.apply(gray)\n",
        "\n",
        "    p_mask = np.zeros_like(gray)\n",
        "\n",
        "    # [ìˆ˜ì •] 8: ì™¼ìª½ ëˆˆ, 9: ì˜¤ë¥¸ìª½ ëˆˆ ì˜ì—­ë§Œ íƒ€ê²ŸíŒ…\n",
        "    for eye_id in [4, 5]:\n",
        "        mask_np = gt_mask.cpu().numpy() if torch.is_tensor(gt_mask) else gt_mask\n",
        "        eye_area = (mask_np == eye_id).astype(np.uint8)\n",
        "\n",
        "        if np.sum(eye_area) == 0: continue\n",
        "\n",
        "        # ëˆˆ ì˜ì—­ ë‚´ í•˜ìœ„ 35% ë°ê¸°ë¥¼ ì„ê³„ê°’ìœ¼ë¡œ ì„¤ì • (ë” ë„“ê²Œ ì¡ê¸°)\n",
        "        eye_pixels = enhanced_gray[eye_area > 0]\n",
        "        thresh_val = np.percentile(eye_pixels, 35)\n",
        "\n",
        "        _, p_bin = cv2.threshold(enhanced_gray, thresh_val, 255, cv2.THRESH_BINARY_INV)\n",
        "        p_mask_single = cv2.bitwise_and(p_bin, p_bin, mask=eye_area)\n",
        "\n",
        "        # íŒ½ì°½ ì—°ì‚°ìœ¼ë¡œ ëˆˆë™ì ì˜ì—­ í™•ì¥ (í•œ í”½ì…€ ë” ë‘ê»ê²Œ)\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "        p_mask_single = cv2.dilate(p_mask_single, kernel, iterations=1)\n",
        "\n",
        "        # Convex Hullë¡œ ì‹¤ì œ í˜•íƒœë¥¼ ì‚´ë¦¬ë©° ìì—°ìŠ¤ëŸ½ê²Œ ë³´ì •\n",
        "        contours, _ = cv2.findContours(p_mask_single, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        for cnt in contours:\n",
        "            if cv2.contourArea(cnt) > 15:\n",
        "                hull = cv2.convexHull(cnt)\n",
        "                cv2.drawContours(p_mask, [hull], -1, 255, -1)\n",
        "\n",
        "    return img_np, p_mask\n",
        "\n",
        "# 2. ì‹¤í–‰ ë° ì‹œê°í™”\n",
        "try:\n",
        "    # ìƒ˜í”Œ ë¡œë“œ (datasetì´ ì •ì˜ë˜ì–´ ìˆì–´ì•¼ í•¨)\n",
        "    idx = random.randint(0, len(dataset) - 1)\n",
        "    test_img, test_mask, fname = dataset[idx]\n",
        "\n",
        "    # í•¨ìˆ˜ í˜¸ì¶œ\n",
        "    img_raw, p_detected = detect_pupils_natural_fixed(test_img, test_mask)\n",
        "\n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img_raw)\n",
        "    plt.title(f\"Input Image: {fname}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(img_raw)\n",
        "\n",
        "    # 0ì€ íˆ¬ëª…, 1ì€ í•˜ëŠ˜ìƒ‰(Cyan)ìœ¼ë¡œ ëˆˆë™ìë§Œ í‘œì‹œ\n",
        "    overlay_cmap = mcolors.ListedColormap([(0,0,0,0), 'cyan'])\n",
        "    plt.imshow(p_detected, cmap=overlay_cmap, alpha=0.7)\n",
        "    plt.title(\"Detected Pupils from Ground Truth\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "except NameError:\n",
        "    print(\"ì—ëŸ¬: 'dataset' ë³€ìˆ˜ê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "segfaceë¡œë“œ"
      ],
      "metadata": {
        "id": "RBe4JvMYOQKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì½”ë© ì…€ì— ì´ ì½”ë“œ ë¶™ì´ê¸°\n",
        "import os\n",
        "import cv2\n",
        "from PIL import Image, ImageOps\n",
        "import subprocess\n",
        "\n",
        "# ì›ë³¸ ì´ë¯¸ì§€ ê²½ë¡œ\n",
        "input_image_path = \"/content/test_images/input_sample.jpg\"\n",
        "\n",
        "# EXIF íšŒì „ ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ì €ì¥í•  ì„ì‹œ ê²½ë¡œ\n",
        "fixed_image_path = \"/content/fixed_image_temp.jpg\"\n",
        "\n",
        "# 1ë‹¨ê³„: EXIF ì²˜ë¦¬\n",
        "img = Image.open(input_image_path)\n",
        "img = ImageOps.exif_transpose(img)  # â˜… íšŒì „ ì ìš©\n",
        "img.save(fixed_image_path)\n",
        "\n",
        "print(\"âœ… EXIF ì²˜ë¦¬ + ì¶”ë¡  ì™„ë£Œ!\")\n"
      ],
      "metadata": {
        "id": "9EFV4Ce_P9n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. SegFace ì €ì¥ì†Œ í´ë¡  ë° ì´ë™\n",
        "!git clone https://github.com/Kartik-3004/SegFace.git\n",
        "%cd SegFace\n",
        "\n",
        "# 2. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ì½”ë© í™˜ê²½ ìµœì í™” ë²„ì „)\n",
        "!pip install -q timm==0.9.12 segmentation-models-pytorch albumentations python-dotenv huggingface_hub"
      ],
      "metadata": {
        "id": "eO4TYw8bORy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# 1. .env ê²½ë¡œ ì„¤ì • íŒŒì¼ ìƒì„±\n",
        "root_path = \"/content/SegFace\"\n",
        "with open(\".env\", \"w\") as f:\n",
        "    f.write(f\"ROOT_PATH={root_path}\\n\")\n",
        "    f.write(f\"DATA_PATH={root_path}/data\\n\")\n",
        "    f.write(f\"LOG_PATH={root_path}/logs\\n\")\n",
        "\n",
        "# 2. ê°€ì¤‘ì¹˜(Weights) ë‹¤ìš´ë¡œë“œ\n",
        "hf_hub_download(repo_id=\"kartiknarayan/SegFace\",\n",
        "                filename=\"convnext_celeba_512/model_299.pt\",\n",
        "                local_dir=\"./weights\")\n",
        "\n",
        "print(\"âœ… í™˜ê²½ ì„¤ì • ë° ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")"
      ],
      "metadata": {
        "id": "zxEoxZjBOTeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import sys\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸ ê²½ë¡œ ì¶”ê°€\n",
        "if '/content/SegFace' not in sys.path:\n",
        "    sys.path.append('/content/SegFace')\n",
        "from network.models.segface_celeb import SegFaceCeleb\n",
        "\n",
        "# --- [ì„¤ì • ì˜ì—­] ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_path = \"/content/SegFace/weights/convnext_celeba_512/model_299.pt\"\n",
        "input_res = 512\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸í•  ë°ì´í„° ê²½ë¡œ (ë³¸ì¸ì˜ ê²½ë¡œë¡œ ìˆ˜ì •í•˜ì„¸ìš”)\n",
        "# ì˜ˆ: /content/drive/MyDrive/test_data/images\n",
        "test_images_path = \"/content/drive/MyDrive/c. Final_Team/Split_dataset/test/images\"\n",
        "test_masks_path = \"/content/drive/MyDrive/c. Final_Team/Split_dataset/test/masks\"\n",
        "\n",
        "# ìˆ˜ì •ëœ class_names (SegFace ëª¨ë¸ ì¶œë ¥ ìˆœì„œ)\n",
        "class_names = [\n",
        "    'bg', 'neck', 'face', 'cloth', 'lr', 'rr', 'lb', 'rb', 'le',\n",
        "    're', 'nose', 'imouth', 'llip', 'ulip', 'hair',\n",
        "    'glass', 'hat', 'earr', 'neckl'\n",
        "]\n",
        "# ------------------\n",
        "\n",
        "\n",
        "\n",
        "AI_HUB_TO_SEGFACE =  {\n",
        "    0: 0, 1: 2, 2: 7, 3: 6, 4: 9, 5: 8, 6: 15, 7: 5, 8: 4, 9: 17,\n",
        "    10: 10, 11: 11, 12: 13, 13: 12, 14: 1, 15: 18, 16: 3, 17: 14, 18: 16,\n",
        "}\n",
        "\n",
        "# 1. ëª¨ë¸ ë¡œë“œ í•¨ìˆ˜\n",
        "def load_segface_model():\n",
        "    model = SegFaceCeleb(input_res, \"convnext_base\")\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    pretrained_dict = checkpoint.get('state_dict_backbone', checkpoint.get('model_state_dict', checkpoint))\n",
        "    model.load_state_dict(pretrained_dict, strict=False)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# 2. ë‹¤ì¤‘ ë°ì´í„° ë¡œë” í´ë˜ìŠ¤\n",
        "class SegFaceEvalDataset(Dataset):\n",
        "    def __init__(self, img_dir, mask_dir, res=512):\n",
        "        import glob\n",
        "        import os\n",
        "        self.img_paths = sorted(glob.glob(os.path.join(img_dir, \"*.*\")))\n",
        "        self.mask_paths = sorted(glob.glob(os.path.join(mask_dir, \"*.*\")))\n",
        "        self.res = res\n",
        "    def __len__(self): return len(self.img_paths)\n",
        "\n",
        "    def align_aihub_labels(self, mask_np):\n",
        "        \"\"\"AI í—ˆë¸Œ IDë¥¼ SegFace IDë¡œ ì¬ë°°ì—´\"\"\"\n",
        "        new_mask = np.zeros_like(mask_np)\n",
        "        for ai_id, seg_id in AI_HUB_TO_SEGFACE.items():\n",
        "            new_mask[mask_np == ai_id] = seg_id\n",
        "        return new_mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 1. ì´ë¯¸ì§€ ë¡œë“œ\n",
        "        img = Image.open(self.img_paths[idx]).convert('RGB').resize((self.res, self.res))\n",
        "        img_tensor = torch.from_numpy(np.array(img)).permute(2, 0, 1).float().div(255)\n",
        "\n",
        "        # 2. ë§ˆìŠ¤í¬ ë¡œë“œ (AI í—ˆë¸Œ Grayscale ëŒ€ì‘)\n",
        "        mask = Image.open(self.mask_paths[idx]).convert('L')\n",
        "        mask = mask.resize((self.res, self.res), Image.NEAREST)\n",
        "        mask_np = np.array(mask).astype(np.int64)\n",
        "\n",
        "        # ğŸ’¡ [í•µì‹¬] AI í—ˆë¸Œ ë°ì´í„° ì „ì²˜ë¦¬\n",
        "        # ë§Œì•½ AI í—ˆë¸Œ ë°ì´í„°ê°€ 0, 1, 2... ìˆœì„œê°€ ì•„ë‹ˆë¼\n",
        "        # ì‹œê°í™”ë¥¼ ìœ„í•´ í° ê°’(ì˜ˆ: 10, 20...)ì„ ê°€ì§€ê³  ìˆë‹¤ë©´ ì •ê·œí™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n",
        "        # ê³ ìœ ê°’ì„ í™•ì¸í•œ í›„ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.\n",
        "        mask_np = self.align_aihub_labels(mask_np)\n",
        "\n",
        "        mask_tensor = torch.from_numpy(mask_np).long()\n",
        "        return img_tensor, mask_tensor, os.path.basename(self.img_paths[idx])\n",
        "\n"
      ],
      "metadata": {
        "id": "xyv6oI8FO3Yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "from PIL import Image\n",
        "\n",
        "def process_new_input(image_path, model, device, res=512):\n",
        "    # 1. ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
        "    orig_img = Image.open(image_path).convert('RGB')\n",
        "    img_resized = orig_img.resize((res, res))\n",
        "\n",
        "    # [B, C, H, W] í˜•íƒœë¡œ ë³€í™˜\n",
        "    img_tensor = torch.from_numpy(np.array(img_resized)).permute(2, 0, 1).float().div(255).unsqueeze(0).to(device)\n",
        "\n",
        "    # 2. SegFace ì¶”ë¡  (forward ì—ëŸ¬ í•´ê²°)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # [ìˆ˜ì • í¬ì¸íŠ¸] SegFace forwardëŠ” (img, labels, dataset)ì„ ìš”êµ¬í•¨\n",
        "        # ì¶”ë¡  ì‹œì—ëŠ” labelsì™€ datasetì— Noneì´ë‚˜ ì„ì˜ì˜ ê°’ì„ ì „ë‹¬\n",
        "        output = model(img_tensor, labels=None, dataset=None)\n",
        "\n",
        "        # ëª¨ë¸ ì¶œë ¥ êµ¬ì¡° ëŒ€ì‘ (ì¼ë°˜ì ìœ¼ë¡œ 0ë²ˆ ì¸ë±ìŠ¤ê°€ ìµœì¢… ë¡œì§“)\n",
        "        if isinstance(output, (list, tuple)):\n",
        "            output = output[0]\n",
        "\n",
        "        pred_mask = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    # 3. ëˆˆë™ì ì •ë°€ ì¶”ì¶œ ë¡œì§\n",
        "    img_np = (img_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
        "    gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
        "    enhanced_gray = clahe.apply(gray)\n",
        "\n",
        "    pupil_mask = np.zeros_like(gray)\n",
        "\n",
        "    # 8(le), 9(re) ì˜ì—­ íƒ€ê²ŸíŒ…\n",
        "    for eye_id in [8, 9]:\n",
        "        eye_area = (pred_mask == eye_id).astype(np.uint8)\n",
        "        if np.sum(eye_area) == 0: continue\n",
        "\n",
        "        eye_pixels = enhanced_gray[eye_area > 0]\n",
        "        thresh_val = np.percentile(eye_pixels, 35)\n",
        "\n",
        "        _, p_bin = cv2.threshold(enhanced_gray, thresh_val, 255, cv2.THRESH_BINARY_INV)\n",
        "        p_mask_single = cv2.bitwise_and(p_bin, p_bin, mask=eye_area)\n",
        "\n",
        "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
        "        p_mask_single = cv2.dilate(p_mask_single, kernel, iterations=1)\n",
        "\n",
        "        contours, _ = cv2.findContours(p_mask_single, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        for cnt in contours:\n",
        "            if cv2.contourArea(cnt) > 15:\n",
        "                hull = cv2.convexHull(cnt)\n",
        "                cv2.drawContours(pupil_mask, [hull], -1, 255, -1)\n",
        "\n",
        "    return img_np, pred_mask, pupil_mask\n",
        "\n",
        "# --- ì‹¤í–‰ ì½”ë“œ ---\n",
        "sample_image = '/content/test_images/fixed_image_temp.jpg'\n",
        "# sample_image ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸ í›„ ì‹¤í–‰í•˜ì„¸ìš”.\n",
        "img_raw, p_mask, pupil_final = process_new_input(sample_image, model, device)"
      ],
      "metadata": {
        "id": "j66VfwWAK6Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import numpy as np\n",
        "\n",
        "def visualize_pupil_results(img_raw, pred_mask, pupil_final, title=\"Detection Result\"):\n",
        "    \"\"\"\n",
        "    img_raw: ì „ì²˜ë¦¬ëœ ì›ë³¸ ì´ë¯¸ì§€ (Numpy)\n",
        "    pred_mask: SegFaceê°€ ì˜ˆì¸¡í•œ ì „ì²´ ë§ˆìŠ¤í¬\n",
        "    pupil_final: ìµœì¢… ì •ë°€ ì¶”ì¶œëœ ëˆˆë™ì ë§ˆìŠ¤í¬\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(20, 7))\n",
        "\n",
        "    # 1. ì›ë³¸ ì´ë¯¸ì§€\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(img_raw)\n",
        "    plt.title(\"1. Original Input\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # 2. SegFace ëˆˆ ì˜ì—­ (Step 1)\n",
        "    # 8(ì™¼ìª½ ëˆˆ), 9(ì˜¤ë¥¸ìª½ ëˆˆ) ì˜ì—­ì„ ë¶„í™ìƒ‰ ê³„ì—´ë¡œ í‘œì‹œ\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(img_raw)\n",
        "    eye_area = np.isin(pred_mask, [8, 9])\n",
        "\n",
        "    # ëˆˆ ì˜ì—­ë§Œ ê°•ì¡°í•˜ê¸° ìœ„í•œ ë§ˆìŠ¤í¬ ìƒì„±\n",
        "    eye_overlay = np.zeros_like(img_raw, dtype=np.uint8)\n",
        "    eye_overlay[eye_area] = [255, 0, 255] # Magenta ìƒ‰ìƒ\n",
        "\n",
        "    plt.imshow(eye_overlay, alpha=0.4) # íˆ¬ëª…ë„ 0.4ë¡œ ê²¹ì¹˜ê¸°\n",
        "    plt.title(\"2. SegFace Eye Segmentation\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # 3. ì •ë°€ ì¶”ì¶œëœ ëˆˆë™ì (Step 2)\n",
        "    # ìµœì¢… ê²°ê³¼ë¬¼ì¸ pupil_finalì„ í•˜ëŠ˜ìƒ‰(Cyan)ìœ¼ë¡œ ê°•ì¡°\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(img_raw)\n",
        "\n",
        "    # 0ë²ˆ(ë°°ê²½)ì€ íˆ¬ëª…í•˜ê²Œ, 1ë²ˆ(ëˆˆë™ì)ì€ Cyanìƒ‰ìœ¼ë¡œ ì„¤ì •\n",
        "    pupil_cmap = mcolors.ListedColormap([(0,0,0,0), 'cyan'])\n",
        "    plt.imshow(pupil_final, cmap=pupil_cmap, alpha=0.8)\n",
        "\n",
        "    plt.title(\"3. Refined Natural Pupils\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --- ì‹¤í–‰ë¶€ ---\n",
        "# ì•ì„œ ì‹¤í–‰í•œ process_new_inputì˜ ê²°ê³¼ê°’ì„ ê·¸ëŒ€ë¡œ ë„£ìœ¼ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "visualize_pupil_results(img_raw, p_mask, pupil_final, title=f\"Result for: {os.path.basename(sample_image)}\")"
      ],
      "metadata": {
        "id": "24ODOmvaOA4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y7GsSx2HP1wz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
